{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvEOGvE4dmca"
      },
      "source": [
        "## Variaciones sobre el modelo inicial\n",
        "\n",
        "Vamos a implementar alguna modificación sobre el modelo inicial propuesto en la primera sesión.\n",
        "\n",
        "Para empezar vamos a definir un conjunto de validación. Con este conjunto de calidación extraído del propio conjunto de entrenamiento estimaremos los mejores parámetros. Dejaremos el conjunto de test sólo para estimar el acierto final con estos mejores parámetros.\n",
        "\n",
        "Primero leemos los datos y normalizamos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DecBiWpYdmcd",
        "outputId": "dca36600-19ae-4c8b-d074-8d09532e8fa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "training set (60000, 28, 28)\n",
            "test set (10000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print('training set', x_train.shape)\n",
        "print('test set', x_test.shape)\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalize [0..255]-->[0..1]\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "num_classes=10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppKj_72xdmcf"
      },
      "source": [
        "## Partición training/validación\n",
        "\n",
        "Nos quedaremos con un 80% de los datos para entrenar y un 20% para validar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "er8NkjMPdmcg",
        "outputId": "5ae0d7b5-ad81-4a9a-f00a-a859219e9e14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training set (48000, 784)\n",
            "val set (12000, 784)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "print('training set', x_train.shape)\n",
        "print('val set', x_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "art2VGJkdmch"
      },
      "source": [
        "## Experimentación con conjunto de validación\n",
        "\n",
        "Cualquier parámetro a modificar en nuestro modelo debería ser probado sobre el conjunto de validación y escoger aquel que produjera el mejor resultado para probar ese (y sólo ese) sobre el conjunto de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-lgmol_dmci"
      },
      "outputs": [],
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "batch_size=128\n",
        "epochs=5\n",
        "\n",
        "hdim=[128,256,512,1024]\n",
        "best_acc=0.0\n",
        "for h in hdim:\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Input(784))\n",
        "    model.add(Dense(h, activation='relu')) # <-- repetir esta línea tantas veces como número de capas ocultas se quiera\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    sgd=SGD(learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "    history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_val, y_val)) ## <--- OJO validation set\n",
        "\n",
        "    if history.history['val_accuracy'][-1]>best_acc:\n",
        "        best_acc=history.history['val_accuracy'][-1]\n",
        "        besth=h\n",
        "\n",
        "print(\"=============================\")\n",
        "print(\"Best acc\",best_acc)\n",
        "print(\"Best hidden dim\",besth)\n",
        "print(\"=============================\")\n",
        "\n",
        "\n",
        "## PROBAR EL MEJOR CON TEST\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Input(784))\n",
        "model.add(Dense(besth, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "sgd=SGD(learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, # <--- aquí podríamos concatenar trainin+valid para no malgastar datos\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK72keXudmcj"
      },
      "source": [
        "## EARLY STOPPING\n",
        "\n",
        "De la multitud de parámetros a decidir en la definición y entrenamiento de las redes neuronales hay uno que podríamos automatizar. Es el número de epochs a emplear. Dado que tenemos un conjunto de validación vamos a emplearlo para monitorizar cómo evoluciona la convergencia. Si en algún momento el descenso por gradiente parece haber alcanzado una zona de meseta sobre dicho conjunto de validación podremos detener el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpCaUWYwdmck"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Input(784))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "sgd=SGD(learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "callback = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=3) ## También podría ser 'loss' sin necesitar un conjunto de validación\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "epochs=100  ## No nos preocupemos, el fit acabará antes por el early_stopping\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=[callback])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSbHg3Cfdmcl"
      },
      "source": [
        "## **Ejercicio**:\n",
        "\n",
        "Entrenar una red neuronal MLP para el problema MNIST. Los valores que se quieren probar son los siguientes:\n",
        "\n",
        "-   Número de capas ocultas [1,2,3]\n",
        "-   Dimensión de las capas ocultas [512,1024]\n",
        "-   Learning rate [0.025, 0.01]\n",
        "-   Batch_size [128]\n",
        "-   Epochs con early_stopping\n",
        "\n",
        "Obtener la mejor combinación con el conjunto de validación y probarla finalmente sobre el conjunto de test.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC-4DiOmdmcm"
      },
      "source": [
        "## Model checkpoint\n",
        "\n",
        "Los model checkpoint permiten almacenar el estado del modelo en un punto intermedio del entrenamiento. Esto es útil para poder recuperar el modelo en caso de que el entrenamiento se interrumpa por cualquier motivo. Así como para poder recuperar el mejor modelo obtenido durante el entrenamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIeiCe3_dmcn"
      },
      "outputs": [],
      "source": [
        "batch_size=128\n",
        "epochs=100\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Input(784))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "sgd=SGD(learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "callback = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=3)\n",
        "\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=[callback, checkpoint]) ## <--- dos callbacks, el early_stopping y el model_checkpoint\n",
        "\n",
        "\n",
        "## Cargar el mejor modelo y evaluarlo con el test set\n",
        "model = keras.models.load_model('best_model.h5')\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8Xr0hhqdmco"
      },
      "source": [
        "## Ajustar parámetros\n",
        "\n",
        "Es posible que en el ejemplo anterior el entrenamiento haya acabado prematuramente por el efecto del Early Stopping. En ese caso, el modelo que se ha obtenido no es el mejor posible. Para obtener el mejor modelo quizás sea necesario aumentar el número de epochs y para ello modificar algún parámetro del early stopping.\n",
        "\n",
        "## Ejercicio:\n",
        "\n",
        "Modificar el early stopping y emplear un modelo con los mejores parámetros (número de capas ocultas, learning rate, batch size, etc) para evaluar finalmente el modelo guardado sobre el test.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}